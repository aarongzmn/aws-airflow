{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-07-25 22:25:30+0000'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dateutil import parser\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor, execute_values\n",
    "psycopg2.extensions.register_adapter(dict, psycopg2.extras.Json)\n",
    "from envars import envars\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "class ParseTweet():\n",
    "    \"\"\"Use this to parse the tweet scrape object from the 'snscrape' library.\n",
    "        - This will rename the dictionary keys so they are compatible with the database tables.\n",
    "        - These lists will be used to update the 'tweets' and 'users' tablees.\n",
    "    \"\"\"\n",
    "    def __init__(self, tweet, query_id, snapshot_date):\n",
    "        self.tweet_dict = tweet\n",
    "        self.query_id = query_id\n",
    "        self.snapshot_date = snapshot_date\n",
    "\n",
    "    def rename_keys_for_tweet_data(self):\n",
    "        \"\"\"Rename keys to match 'twitterdb' 'tweets' table\n",
    "        \"\"\"\n",
    "        self.tweet_dict[\"url\"] = self.tweet_dict.pop('url', None)\n",
    "        self.tweet_dict[\"tweet_date\"] = self.tweet_dict.pop('date', None)\n",
    "        self.tweet_dict[\"tweet_content\"] = self.tweet_dict.pop('content', None)\n",
    "        self.tweet_dict[\"tweet_rendered_content\"] = self.tweet_dict.pop('renderedContent', None)\n",
    "        self.tweet_dict[\"id\"] = self.tweet_dict.pop('id', None)\n",
    "        self.tweet_dict[\"user_id\"] = self.tweet_dict.pop('user', None)\n",
    "        self.tweet_dict[\"reply_count\"] = self.tweet_dict.pop('replyCount', None)\n",
    "        self.tweet_dict[\"retweet_count\"] = self.tweet_dict.pop('retweetCount', None)\n",
    "        self.tweet_dict[\"like_count\"] = self.tweet_dict.pop('likeCount', None)\n",
    "        self.tweet_dict[\"quote_count\"] = self.tweet_dict.pop('quoteCount', None)\n",
    "        self.tweet_dict[\"conversation_id\"] = self.tweet_dict.pop('conversationId', None)\n",
    "        self.tweet_dict[\"lang\"] = self.tweet_dict.pop('lang', None)\n",
    "        self.tweet_dict[\"tweet_source\"] = self.tweet_dict.pop('source', None)\n",
    "        self.tweet_dict[\"source_url\"] = self.tweet_dict.pop('sourceUrl', None)\n",
    "        self.tweet_dict[\"source_label\"] = self.tweet_dict.pop('sourceLabel', None)\n",
    "        self.tweet_dict[\"outlinks\"] = self.tweet_dict.pop('outlinks', None)\n",
    "        self.tweet_dict[\"tco_outlinks\"] = self.tweet_dict.pop('tcooutlinks', None)\n",
    "        self.tweet_dict[\"media\"] = self.tweet_dict.pop('media', None)\n",
    "        self.tweet_dict[\"retweeted_tweet\"] = self.tweet_dict.pop('retweetedTweet', None)\n",
    "        self.tweet_dict[\"quoted_tweet\"] = self.tweet_dict.pop('quotedTweet', None)\n",
    "        self.tweet_dict[\"in_reply_to_tweet_id\"] = self.tweet_dict.pop('inReplyToTweetId', None)\n",
    "        self.tweet_dict[\"in_reply_to_user\"] = self.tweet_dict.pop('inReplyToUser', None)\n",
    "        self.tweet_dict[\"mentioned_users\"] = self.tweet_dict.pop('mentionedUsers', None)\n",
    "        self.tweet_dict[\"coordinates\"] = self.tweet_dict.pop('coordinates', None)\n",
    "        self.tweet_dict[\"place\"] = self.tweet_dict.pop('place', None)\n",
    "        self.tweet_dict[\"hashtags\"] = self.tweet_dict.pop('hashtags', None)\n",
    "        self.tweet_dict[\"cashtags\"] = self.tweet_dict.pop('cashtags', None)\n",
    "        self.tweet_dict[\"query_id\"] = self.query_id\n",
    "        return self.tweet_dict\n",
    "\n",
    "    def rename_keys_for_user_data(self, users_list):\n",
    "        \"\"\"Rename keys to match 'twitterdb' 'users' table\n",
    "        These data for these keys are all sourced from snscrape library scrape data.\n",
    "        \"\"\"\n",
    "        users_list[\"username\"] = users_list.pop(\"username\")\n",
    "        users_list[\"id\"] = users_list.pop(\"id\")\n",
    "        users_list[\"display_name\"] = users_list.pop(\"displayname\")\n",
    "        users_list[\"description\"] = users_list.pop(\"description\")\n",
    "        users_list[\"raw_description\"] = users_list.pop(\"rawDescription\")\n",
    "        users_list[\"description_urls\"] = users_list.pop(\"descriptionUrls\")\n",
    "        users_list[\"verified\"] = users_list.pop(\"verified\")\n",
    "        users_list[\"created\"] = users_list.pop(\"created\")\n",
    "        users_list[\"followers_count\"] = users_list.pop(\"followersCount\")\n",
    "        users_list[\"friends_count\"] = users_list.pop(\"friendsCount\")\n",
    "        users_list[\"statuses_count\"] = users_list.pop(\"statusesCount\")\n",
    "        users_list[\"favourites_count\"] = users_list.pop(\"favouritesCount\")\n",
    "        users_list[\"listed_count\"] = users_list.pop(\"listedCount\")\n",
    "        users_list[\"media_count\"] = users_list.pop(\"mediaCount\")\n",
    "        users_list[\"location\"] = users_list.pop(\"location\")\n",
    "        users_list[\"protected\"] = users_list.pop(\"protected\")\n",
    "        users_list[\"link_url\"] = users_list.pop(\"linkUrl\")\n",
    "        users_list[\"link_t_courl\"] = users_list.pop(\"linkTcourl\")\n",
    "        users_list[\"profile_image_url\"] = users_list.pop(\"profileImageUrl\")\n",
    "        users_list[\"profile_banner_url\"] = users_list.pop(\"profileBannerUrl\")\n",
    "        users_list[\"label\"] = users_list.pop(\"label\")\n",
    "        users_list[\"user_type\"] = users_list.pop(\"user_type\")\n",
    "        users_list[\"snapshot_date\"] = users_list.pop(\"snapshot_date\")\n",
    "        return users_list\n",
    "\n",
    "    def split_tweet_data_and_user_data(self) -> (dict, dict):\n",
    "        tweet_dict = self.rename_keys_for_tweet_data()\n",
    "        users_list = []\n",
    "\n",
    "        user_id = tweet_dict.pop(\"user_id\")\n",
    "        if user_id:\n",
    "            user_id[\"user_type\"] = \"author\"\n",
    "            users_list.append(user_id)\n",
    "            tweet_dict[\"user_id\"] = user_id[\"id\"]\n",
    "        else:\n",
    "            tweet_dict[\"user_id\"] = None\n",
    "\n",
    "        reply_to = tweet_dict.pop(\"in_reply_to_user\")\n",
    "        if reply_to:\n",
    "            reply_to[\"user_type\"] = \"reply_to\"\n",
    "            users_list.append(reply_to)\n",
    "            tweet_dict[\"in_reply_to_user\"] = reply_to[\"id\"]\n",
    "        else:\n",
    "            tweet_dict[\"in_reply_to_user\"] = None\n",
    "\n",
    "        mentioned_list = tweet_dict.pop(\"mentioned_users\")\n",
    "        if mentioned_list:\n",
    "            tweet_dict[\"mentioned_users\"] = [i[\"id\"] for i in mentioned_list]\n",
    "            for mentioned in mentioned_list:\n",
    "                mentioned[\"user_type\"] = \"mentioned\"\n",
    "                users_list.append(mentioned)\n",
    "        else:\n",
    "            tweet_dict[\"mentioned_users\"] = None\n",
    "        # Add these keys/values to all items in 'users_list'\n",
    "        for user in users_list:\n",
    "            user[\"tweet_id\"] = tweet_dict[\"id\"]\n",
    "            user[\"query_id\"] = self.query_id\n",
    "            user[\"snapshot_date\"] = self.snapshot_date\n",
    "            self.rename_keys_for_user_data(user)\n",
    "        return tweet_dict, users_list\n",
    "\n",
    "\n",
    "def list_objects_in_bucket(bucket_name) -> dict:\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "    bucket_files = response[\"Contents\"]\n",
    "    return bucket_files\n",
    "\n",
    "\n",
    "def read_from_s3(bucket_name:str, key_name:str) -> dict:\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3_object = s3.Object(bucket_name, key_name)\n",
    "    response = s3_object.get()\n",
    "    response_str = response[\"Body\"].read().decode(\"utf-8\")\n",
    "    dict_obj = json.loads(response_str)\n",
    "    return dict_obj\n",
    "\n",
    "\n",
    "def batch_insert_into_database(table_name: str, record_list: [dict]) -> list:\n",
    "    \"\"\"BULK INSERT INTO database (1000 rows at a time). Input list shoud be items in dictionary format.\n",
    "    \"\"\"\n",
    "    col_names = \", \".join(record_list[0].keys())\n",
    "    insert_values = [tuple(e.values()) for e in record_list]\n",
    "    with psycopg2.connect(\n",
    "        dbname=\"twitterdb\",\n",
    "        user=envars.get(\"user\"),\n",
    "        password=envars.get(\"password\"),\n",
    "        host=envars.get(\"host\"),\n",
    "        port=envars.get(\"port\")) as conn:\n",
    "        with conn.cursor(cursor_factory=RealDictCursor) as curs:\n",
    "            sql = f\"INSERT INTO {table_name} ({col_names}) VALUES %s\"\n",
    "            insert_result = psycopg2.extras.execute_values(curs, sql, insert_values, page_size=1000)\n",
    "    return\n",
    "\n",
    "\n",
    "bucket_name = \"twitter-scrape-results\"\n",
    "bucket_files = list_objects_in_bucket(bucket_name)\n",
    "\n",
    "for file in tqdm(bucket_files):\n",
    "    snapshot_date = file[\"LastModified\"]\n",
    "    query_id = int(file[\"Key\"].split(\"_\")[2])\n",
    "    dict_obj = read_from_s3(bucket_name, file[\"Key\"])\n",
    "    tweets_updates = []\n",
    "    users_updates = []\n",
    "    for tweet in dict_obj:\n",
    "        tweets_dict, users_dict = ParseTweet(tweet, query_id, snapshot_date).split_tweet_data_and_user_data()\n",
    "        tweets_updates.append(tweets_dict)\n",
    "        users_updates.extend(users_dict)\n",
    "    batch_insert_into_database(\"tweets\", tweets_updates)\n",
    "    batch_insert_into_database(\"users\", users_updates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envars import envars\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor, execute_values\n",
    "psycopg2.extensions.register_adapter(dict, psycopg2.extras.Json)\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def select_from_database(sql):\n",
    "    with psycopg2.connect(\n",
    "        dbname=\"twitterdb\",\n",
    "        user=envars.get(\"user\"),\n",
    "        password=envars.get(\"password\"),\n",
    "        host=envars.get(\"host\"),\n",
    "        port=envars.get(\"port\")) as conn:\n",
    "        with conn.cursor(cursor_factory=RealDictCursor) as curs:\n",
    "            curs.execute(f\"{sql}\")\n",
    "            query_result = curs.fetchall()\n",
    "            query_dicts = [dict(row) for row in query_result]\n",
    "            return query_dicts\n",
    "\n",
    "\n",
    "sql = \"SELECT * FROM queries WHERE active is true\"\n",
    "query_tasks = select_from_database(sql)\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT query_id, CAST(tweet_date as DATE), COUNT(*)\n",
    "FROM tweets\n",
    "GROUP BY query_id, CAST(tweet_date as DATE)\n",
    "ORDER BY tweet_date ASC\n",
    "\"\"\"\n",
    "days_scraped = select_from_database(sql)\n",
    "\n",
    "max_scrape_date = datetime.now(timezone.utc) - timedelta(days=2)  # limit scrape to data > 1 day old\n",
    "for i in range(len(query_tasks)):\n",
    "    db_scrape_data = [q[\"tweet_date\"].strftime(\"%Y-%m-%d\") for q in days_scraped if q[\"query_id\"] == query_tasks[i][\"id\"]]\n",
    "\n",
    "    scrape_from = query_tasks[i][\"scrape_from_date\"]\n",
    "    scrape_to = min(query_tasks[i][\"scrape_to_date\"], max_scrape_date)\n",
    "    date_range_dt = pd.date_range(start=scrape_from, end=scrape_to)\n",
    "    date_range_str = list(date_range_dt.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    query_tasks[i][\"to_scrape\"] = [date for date in date_range_str if date not in db_scrape_data]\n",
    "    del query_tasks[i][\"scrape_from_date\"]\n",
    "    del query_tasks[i][\"scrape_to_date\"]\n",
    "\n",
    "len(query_tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def query_factory(query_template, date_list):\n",
    "    query_list = []\n",
    "    for date in date_list:\n",
    "        start_date = date.strftime(\"%Y-%m-%d\")\n",
    "        end_date = (date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "        query = query_template.replace(\"$STARTDATE\", start_date).replace(\"$ENDDATE\", end_date)\n",
    "        query_list.append(\n",
    "            {   \n",
    "                \"start_date\": start_date,\n",
    "                \"query\": query\n",
    "            }\n",
    "        )\n",
    "    return query_list\n",
    "\n",
    "query_template = \"Example Query start=$STARTDATE end=$ENDDATE\"\n",
    "date_list = pd.date_range(start=\"2022-01-01\", end=\"2022-02-01\")#.strftime(\"%Y-%m-%d\")\n",
    "query_list = query_factory(query_template, date_list)\n",
    "query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "url = \"https://httpbin.org/post\"\n",
    "data = {\n",
    "    \"name\": \"aaron\"\n",
    "}\n",
    "r = requests.post(url, json=data)\n",
    "r.json()[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Queries\n",
    "### GET Latest Record for each Query\n",
    "```\n",
    "SELECT c.*, p1.*\n",
    "FROM queries c\n",
    "JOIN tweets p1 ON (c.id = p1.queryid)\n",
    "LEFT OUTER JOIN tweets p2 ON (c.id = p2.queryid AND \n",
    "    (p1.tweetdate < p2.tweetdate OR (p1.tweetdate = p2.tweetdate AND p1.queryid < p2.queryid)))\n",
    "WHERE p2.queryid IS NULL;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Connection with psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envars import envars\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor, execute_values\n",
    "psycopg2.extensions.register_adapter(dict, psycopg2.extras.Json)\n",
    "\n",
    "\n",
    "\n",
    "def select_from_database(db_name, sql):\n",
    "    with psycopg2.connect(\n",
    "        dbname=\"twitterdb\",\n",
    "        user=envars.get(\"user\"),\n",
    "        password=envars.get(\"password\"),\n",
    "        host=envars.get(\"host\"),\n",
    "        port=envars.get(\"port\")) as conn:\n",
    "        with conn.cursor(cursor_factory=RealDictCursor) as curs:\n",
    "            curs.execute(f\"{sql}\")\n",
    "            query_result = curs.fetchall()\n",
    "            query_dicts = [dict(row) for row in query_result]\n",
    "            return query_dicts\n",
    "\n",
    "\n",
    "db_name = \"twitterdb\"\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "  query_id,\n",
    "  CAST(tweet_date AS date)\n",
    "FROM (SELECT\n",
    "  *,\n",
    "  ROW_NUMBER() OVER (PARTITION BY CAST(tweet_date AS date)\n",
    "  ORDER BY tweet_date DESC) AS SN\n",
    "FROM tweets) A\n",
    "WHERE sn = 1\n",
    "\"\"\"\n",
    "\n",
    "response = select_from_database(db_name, sql)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Terms\n",
    "```\n",
    "\"drag queen\" until:$ENDDATE since:$STARTDATE\n",
    "- [] drag (drag until:2022-06-02 since:2022-06-01)\n",
    "- [] drag show*\n",
    "- [] drag shows\n",
    "- [] drag queen\n",
    "- [] drag story hour\n",
    "- [] drag queen story hour\n",
    "- [] drag queen story time\n",
    "\n",
    "(groom OR gr00m) until:$ENDDATE since:$STARTDATE\n",
    "- [] Groom*\n",
    "- [] gr00m*\n",
    "\n",
    "(ped0 OR pedo) until:$ENDDATE since:$STARTDATE\n",
    "- [] ped0\n",
    "- [] pedo\n",
    "\n",
    "(pedophile OR pedophilia) until:$ENDDATE since:$STARTDATE\n",
    "- [] pedophile\n",
    "- [] pedophilia\n",
    "\n",
    "```\n",
    "### snscrape docs\n",
    "    - https://github.com/JustAnotherArchivist/snscrape\n",
    "    - https://github.com/JustAnotherArchivist/snscrape/blob/master/snscrape/modules/twitter.py\n",
    "- Twiter Advanced Search: https://twitter.com/search-advanced\n",
    "- https://github.com/igorbrigadir/twitter-advanced-search\n",
    "\n",
    "### Development Version\n",
    "- pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Scraper (Local Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "# from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dateutil import parser\n",
    "# import copy\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor, execute_values\n",
    "psycopg2.extensions.register_adapter(dict, psycopg2.extras.Json)\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"Aaron Guzman\",\n",
    "    \"depend_on_past\": False,\n",
    "    \"start_date\": datetime(2020, 1, 1),\n",
    "    \"retries\": 0,\n",
    "    \"retry_delay\": timedelta(seconds=30),\n",
    "}\n",
    "\n",
    "\n",
    "class ParseTweet():\n",
    "    \"\"\"Use this to parse the tweet scrape object from the 'snscrape' library.\n",
    "        - This will rename the dictionary keys so they are compatible with the database tables.\n",
    "        - These lists will be used to update the 'tweets' and 'users' tablees.\n",
    "    \"\"\"\n",
    "    def __init__(self, tweet, query_id):\n",
    "        self.tweet_dict = tweet\n",
    "        self.query_id = query_id\n",
    "\n",
    "    def rename_keys_for_tweet_data(self):\n",
    "        \"\"\"Rename keys to match 'twitterdb' 'tweets' table\n",
    "        \"\"\"\n",
    "        self.tweet_dict[\"url\"] = self.tweet_dict.pop('url', None)\n",
    "        self.tweet_dict[\"tweet_date\"] = self.tweet_dict.pop('date', None)\n",
    "        self.tweet_dict[\"tweet_content\"] = self.tweet_dict.pop('content', None)\n",
    "        self.tweet_dict[\"tweet_rendered_content\"] = self.tweet_dict.pop('renderedContent', None)\n",
    "        self.tweet_dict[\"id\"] = self.tweet_dict.pop('id', None)\n",
    "        self.tweet_dict[\"user_id\"] = self.tweet_dict.pop('user', None)\n",
    "        self.tweet_dict[\"reply_count\"] = self.tweet_dict.pop('replyCount', None)\n",
    "        self.tweet_dict[\"retweet_count\"] = self.tweet_dict.pop('retweetCount', None)\n",
    "        self.tweet_dict[\"like_count\"] = self.tweet_dict.pop('likeCount', None)\n",
    "        self.tweet_dict[\"quote_count\"] = self.tweet_dict.pop('quoteCount', None)\n",
    "        self.tweet_dict[\"conversation_id\"] = self.tweet_dict.pop('conversationId', None)\n",
    "        self.tweet_dict[\"lang\"] = self.tweet_dict.pop('lang', None)\n",
    "        self.tweet_dict[\"tweet_source\"] = self.tweet_dict.pop('source', None)\n",
    "        self.tweet_dict[\"source_url\"] = self.tweet_dict.pop('sourceUrl', None)\n",
    "        self.tweet_dict[\"source_label\"] = self.tweet_dict.pop('sourceLabel', None)\n",
    "        self.tweet_dict[\"outlinks\"] = self.tweet_dict.pop('outlinks', None)\n",
    "        self.tweet_dict[\"tco_outlinks\"] = self.tweet_dict.pop('tcooutlinks', None)\n",
    "        self.tweet_dict[\"media\"] = self.tweet_dict.pop('media', None)\n",
    "        self.tweet_dict[\"retweeted_tweet\"] = self.tweet_dict.pop('retweetedTweet', None)\n",
    "        self.tweet_dict[\"quoted_tweet\"] = self.tweet_dict.pop('quotedTweet', None)\n",
    "        self.tweet_dict[\"in_reply_to_tweet_id\"] = self.tweet_dict.pop('inReplyToTweetId', None)\n",
    "        self.tweet_dict[\"in_reply_to_user\"] = self.tweet_dict.pop('inReplyToUser', None)\n",
    "        self.tweet_dict[\"mentioned_users\"] = self.tweet_dict.pop('mentionedUsers', None)\n",
    "        self.tweet_dict[\"coordinates\"] = self.tweet_dict.pop('coordinates', None)\n",
    "        self.tweet_dict[\"place\"] = self.tweet_dict.pop('place', None)\n",
    "        self.tweet_dict[\"hashtags\"] = self.tweet_dict.pop('hashtags', None)\n",
    "        self.tweet_dict[\"cashtags\"] = self.tweet_dict.pop('cashtags', None)\n",
    "        self.tweet_dict[\"query_id\"] = self.query_id\n",
    "        return self.tweet_dict\n",
    "\n",
    "    def rename_keys_for_user_data(self, users_list):\n",
    "        \"\"\"Rename keys to match 'twitterdb' 'users' table\n",
    "        \"\"\"\n",
    "        users_list[\"username\"] = users_list.pop(\"username\")\n",
    "        users_list[\"id\"] = users_list.pop(\"id\")\n",
    "        users_list[\"display_name\"] = users_list.pop(\"displayname\")\n",
    "        users_list[\"description\"] = users_list.pop(\"description\")\n",
    "        users_list[\"raw_description\"] = users_list.pop(\"rawDescription\")\n",
    "        users_list[\"description_urls\"] = users_list.pop(\"descriptionUrls\")\n",
    "        users_list[\"verified\"] = users_list.pop(\"verified\")\n",
    "        users_list[\"created\"] = users_list.pop(\"created\")\n",
    "        users_list[\"followers_count\"] = users_list.pop(\"followersCount\")\n",
    "        users_list[\"friends_count\"] = users_list.pop(\"friendsCount\")\n",
    "        users_list[\"statuses_count\"] = users_list.pop(\"statusesCount\")\n",
    "        users_list[\"favourites_count\"] = users_list.pop(\"favouritesCount\")\n",
    "        users_list[\"listed_count\"] = users_list.pop(\"listedCount\")\n",
    "        users_list[\"media_count\"] = users_list.pop(\"mediaCount\")\n",
    "        users_list[\"location\"] = users_list.pop(\"location\")\n",
    "        users_list[\"protected\"] = users_list.pop(\"protected\")\n",
    "        users_list[\"link_url\"] = users_list.pop(\"linkUrl\")\n",
    "        users_list[\"link_t_courl\"] = users_list.pop(\"linkTcourl\")\n",
    "        users_list[\"profile_image_url\"] = users_list.pop(\"profileImageUrl\")\n",
    "        users_list[\"profile_banner_url\"] = users_list.pop(\"profileBannerUrl\")\n",
    "        users_list[\"label\"] = users_list.pop(\"label\")\n",
    "        users_list[\"user_type\"] = users_list.pop(\"user_type\")\n",
    "        users_list[\"snapshot_date\"] = users_list.pop(\"snapshot_date\")\n",
    "        return users_list\n",
    "\n",
    "    def split_tweet_data_and_user_data(self) -> (dict, dict):\n",
    "        tweet_dict = self.rename_keys_for_tweet_data()\n",
    "        users_list = []\n",
    "        user_id = tweet_dict.pop(\"user_id\")\n",
    "        if user_id:\n",
    "            user_id[\"user_type\"] = \"owner\"\n",
    "            users_list.append(user_id)\n",
    "            tweet_dict[\"user_id\"] = user_id[\"id\"]\n",
    "        else:\n",
    "            tweet_dict[\"user_id\"] = None\n",
    "\n",
    "        reply_to = tweet_dict.pop(\"in_reply_to_user\")\n",
    "        if reply_to:\n",
    "            reply_to[\"user_type\"] = \"reply_to\"\n",
    "            users_list.append(reply_to)\n",
    "            tweet_dict[\"in_reply_to_user\"] = reply_to[\"id\"]\n",
    "        else:\n",
    "            tweet_dict[\"in_reply_to_user\"] = None\n",
    "\n",
    "        mentioned_list = tweet_dict.pop(\"mentioned_users\")\n",
    "        if mentioned_list:\n",
    "            tweet_dict[\"mentioned_users\"] = [i[\"id\"] for i in mentioned_list]\n",
    "            for mentioned in mentioned_list:\n",
    "                mentioned[\"user_type\"] = \"mentioned\"\n",
    "                users_list.append(mentioned)\n",
    "        else:\n",
    "            tweet_dict[\"mentioned_users\"] = None\n",
    "        # Add these keys/values to all items in 'users_list'\n",
    "        for user in users_list:\n",
    "            user[\"snapshot_date\"] = tweet_dict[\"tweet_date\"]\n",
    "            self.rename_keys_for_user_data(user)\n",
    "        return tweet_dict, users_list\n",
    "\n",
    "\n",
    "# HELPER FUNCTION\n",
    "def select_from_database(sql) -> [dict]:\n",
    "    \"\"\"SELECT database and return results in dictionary format.\n",
    "    \"\"\"\n",
    "    conn = PostgresHook(postgres_conn_id=\"aws_twitterdb\").get_conn()\n",
    "    with conn.cursor(cursor_factory=RealDictCursor) as curs:\n",
    "        curs.execute(f\"{sql}\")\n",
    "        query_result = curs.fetchall()\n",
    "        result_list = [dict(row) for row in query_result]\n",
    "    return result_list\n",
    "\n",
    "# HELPER FUNCTION\n",
    "def batch_insert_into_database(table_name: str, record_list: [dict]) -> list:\n",
    "    \"\"\"BULK INSERT INTO database (1000 rows at a time). Input list shoud be items in dictionary format.\n",
    "    \"\"\"\n",
    "    col_names = \", \".join(record_list[0].keys())\n",
    "    insert_values = [tuple(e.values()) for e in record_list]\n",
    "    with PostgresHook(postgres_conn_id=\"aws_twitterdb\").get_conn() as conn:\n",
    "        with conn.cursor() as curs:\n",
    "            sql = f\"INSERT INTO {table_name} ({col_names}) VALUES %s RETURNING id\"\n",
    "            insert_result = psycopg2.extras.execute_values(curs, sql, insert_values, page_size=1000, fetch=True)\n",
    "    return insert_result\n",
    "\n",
    "\n",
    "# TASK 1\n",
    "def get_query_tasks_from_database(**context) -> [dict]:\n",
    "    \"\"\"\n",
    "    1. Use the query start and end date range to creat a list of days for queries that need to be updated.\n",
    "    2. Query the database to make a list of the days already have scrape data in the database.\n",
    "    Compare lists 1 and 2 to figure out which remaining days need scrape data.\n",
    "    \"\"\"\n",
    "    sql = \"SELECT * FROM queries WHERE active is true\"\n",
    "    query_tasks = select_from_database(sql)\n",
    "    for i in range(len(query_tasks)):\n",
    "        query_tasks[i][\"to_scrape\"] = []\n",
    "        # Create a list of days that have at least 1 tweet entry in the tweets table\n",
    "        sql = f\"\"\"\n",
    "            SELECT TWEETS.*\n",
    "            FROM TWEETS\n",
    "            JOIN\n",
    "                (SELECT MIN(T2.TWEET_DATE) AS MIN_TIMESTAMP\n",
    "                    FROM TWEETS T2\n",
    "                    GROUP BY DATE(T2.TWEET_DATE)) T2 ON TWEETS.TWEET_DATE = T2.MIN_TIMESTAMP\n",
    "            AND TWEETS.QUERY_ID = {query_tasks[i][\"id\"]}\n",
    "            \"\"\"\n",
    "        response = select_from_database(sql)\n",
    "        scrape_processed_days = [i[\"tweet_date\"].strftime(\"%Y-%m-%d\") for i in response]\n",
    "\n",
    "        scrape_from = query_tasks[i][\"scrape_from_date\"]\n",
    "        scrape_to = query_tasks[i][\"scrape_to_date\"]\n",
    "        if scrape_to > datetime.now(timezone.utc):\n",
    "            scrape_to = datetime.now(timezone.utc)\n",
    "        date_list = pd.date_range(start=scrape_from, end=scrape_to)\n",
    "        date_list = [i.strftime(\"%Y-%m-%d\") for i in date_list]\n",
    "\n",
    "        for date in date_list:\n",
    "            if date not in scrape_processed_days:\n",
    "                query_tasks[i][\"to_scrape\"].append(date)\n",
    "\n",
    "        query_tasks[i][\"scrape_from_date\"] = scrape_from.strftime(\"%Y-%m-%d\")\n",
    "        query_tasks[i][\"scrape_to_date\"] = scrape_to.strftime(\"%Y-%m-%d\")\n",
    "    context[\"ti\"].xcom_push(key=\"query_tasks\", value=query_tasks)\n",
    "\n",
    "# TASK 2\n",
    "def scrape_tweets_for_query(**context):\n",
    "    \"\"\"Get scrape results for each day for each query.\n",
    "    The scraping job is offloaded to a Lambda function that returns a list of scrape results.\n",
    "    Lastly, the scrape results are INSERTED INTO the database 'tweets' and 'users' tables.\n",
    "    \"\"\"\n",
    "    query_tasks = context[\"ti\"].xcom_pull(task_ids=\"get_query_tasks_from_database\", key=\"query_tasks\")\n",
    "    for query in query_tasks:\n",
    "        date_list = query[\"to_scrape\"]\n",
    "        if len(date_list) == 0:\n",
    "            print(\"No updates found at this time.\")\n",
    "        else:\n",
    "            print(f\"Scraping {len(date_list)} days worth of updates for query ID: {query['id']}: {date_list}\")\n",
    "            query_template = query[\"query_string\"]\n",
    "            for query_date in date_list:\n",
    "                start_date_dt = datetime.strptime(query_date, \"%Y-%m-%d\")\n",
    "                start_date_str = start_date_dt.strftime('%Y-%m-%d')\n",
    "                end_date_dt = (start_date_dt + timedelta(days=1)).date()\n",
    "                end_date_str = end_date_dt.strftime('%Y-%m-%d')\n",
    "                if end_date_dt >= datetime.utcnow().date():\n",
    "                    # Only scrape if all of results are from the previous day\n",
    "                    pass\n",
    "                else:\n",
    "                    query_string = query_template.replace(\"$STARTDATE\", start_date_str).replace(\"$ENDDATE\", end_date_str)\n",
    "                    data = {\"query\": query_string}\n",
    "                    try:\n",
    "                        # the lambda endpoing would be converted to an environment variable in production\n",
    "                        r = requests.post(\"https://66s4jhi0ma.execute-api.us-west-2.amazonaws.com/api/query\", json=data)\n",
    "                        time.sleep(5)\n",
    "                        r.raise_for_status()\n",
    "                        \n",
    "                    except HTTPError as e:\n",
    "                        print(f\"Request failed with status code {r.status_code} while working on date {start_date_str} for query: {query_string}\")\n",
    "                        print(r.json())\n",
    "                        time.sleep(15)\n",
    "                        raise\n",
    "\n",
    "                    query_response = r.json()\n",
    "                    print(f\"Response for {query_string} on {start_date_str}contains {len(query_response)} tweets for query: {query_string}\")\n",
    "\n",
    "                    tweets_table_updates = []\n",
    "                    users_table_updates = []\n",
    "                    query_id = query[\"id\"]\n",
    "                    for tweet in query_response:\n",
    "                        tweets_dict, users_dict = ParseTweet(tweet, query_id).split_tweet_data_and_user_data()\n",
    "                        tweets_table_updates.append(tweets_dict)\n",
    "                        users_table_updates.extend(users_dict)\n",
    "\n",
    "                    insert_result = batch_insert_into_database(\"tweets\", tweets_table_updates)\n",
    "                    print(f\"Added {len(insert_result)} new tweets for query: {query_string}\")\n",
    "                    insert_result = batch_insert_into_database(\"users\", users_table_updates)\n",
    "                    print(f\"Added {len(insert_result)} new users for query: {query_string}\")\n",
    "    return\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"twitter_scrape\",\n",
    "    default_args=default_args,\n",
    "    schedule_interval=\"@hourly\",\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    get_query_tasks_from_database = PythonOperator(\n",
    "        task_id=\"get_query_tasks_from_database\",\n",
    "        python_callable=get_query_tasks_from_database,\n",
    "        provide_context=True,\n",
    "    )\n",
    "    \n",
    "    scrape_tweets_for_query = PythonOperator(\n",
    "        task_id=\"scrape_tweets_for_query\",\n",
    "        python_callable=scrape_tweets_for_query,\n",
    "        provide_context=True,\n",
    "    )\n",
    "    \n",
    "    get_query_tasks_from_database >> scrape_tweets_for_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function URL (Local Dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snscrape.modules import twitter as sntwitter\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import requests\n",
    "import platform\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event):  # windows\n",
    "# def lambda_handler(event, context):  # lambda\n",
    "    limit = None\n",
    "    if event[\"body\"]:\n",
    "        data = json.loads(event[\"body\"])\n",
    "        query_string = data.get(\"query_string\")\n",
    "        if query_string is None:\n",
    "            error_type = \"Missing 'query_string'\"\n",
    "            print(error_type)\n",
    "            return {\"statusCode\": 422, 'body': error_type}\n",
    "\n",
    "        bucket_name = data.get(\"bucket_name\")\n",
    "        if bucket_name is None:\n",
    "            error_type = \"Missing 'bucket_name'\"\n",
    "            print(error_type)\n",
    "            return {\"statusCode\": 422, 'body': error_type}\n",
    "\n",
    "        key_name = data.get(\"key_name\")\n",
    "        if key_name is None:\n",
    "            error_type = \"Missing 'key_name'\"\n",
    "            print(error_type)\n",
    "            return {\"statusCode\": 422, 'body': error_type}\n",
    "\n",
    "        twitter_dicts = []\n",
    "        for tweet in sntwitter.TwitterSearchScraper(query_string).get_items():\n",
    "            twitter_dicts.append(todict(tweet))\n",
    "            if limit and len(twitter_dicts) == limit:\n",
    "                print(f\"Limitting to {limit} results.\")\n",
    "                break\n",
    "        print(f\"Found {len(twitter_dicts)} items for query: {query_string}\")\n",
    "        return twitter_dicts\n",
    "        response = write_to_s3(bucket_name, key_name, twitter_dicts)\n",
    "        return {\"statusCode\": 200, \"body\": f\"Sucessfully processed {len(twitter_dicts)} items for query: '{query_string}'. File can be found in S3.\"}\n",
    "    else:\n",
    "        return {'statusCode': 500, \"body\": \"Request failed, see logs for details.\"}\n",
    "\n",
    "\n",
    "def write_to_s3(bucket_name:str, key_name:str, upload_dict:dict) -> dict:\n",
    "    \"\"\"Upload Python dictionary to S3\n",
    "    \"\"\"\n",
    "    twitter_bytes = json.dumps(upload_dict).encode('utf-8')\n",
    "    s3 = boto3.resource('s3')\n",
    "    object = s3.Object(bucket_name, key_name)\n",
    "    return object.put(Body=twitter_bytes)\n",
    "\n",
    "\n",
    "def todict(obj, classkey=None) -> dict:\n",
    "    \"\"\"Recursively convert object to dictionary.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        data = {}\n",
    "        for (k, v) in obj.items():\n",
    "            data[k] = todict(v, classkey)\n",
    "        return data\n",
    "    elif hasattr(obj, \"_ast\"):\n",
    "        return todict(obj._ast())\n",
    "    elif hasattr(obj, \"__iter__\") and not isinstance(obj, str):\n",
    "        return [todict(v, classkey) for v in obj]\n",
    "    elif hasattr(obj, \"__dict__\"):\n",
    "        data = dict([(key, todict(value, classkey)) \n",
    "            for key, value in obj.__dict__.items() \n",
    "            if not callable(value) and not key.startswith('_')])\n",
    "        if classkey is not None and hasattr(obj, \"__class__\"):\n",
    "            data[classkey] = obj.__class__.__name__\n",
    "        return data\n",
    "    elif isinstance(obj, datetime):\n",
    "        return obj.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "query_string = '\"drag queen\" until:2022-03-18 since:2022-03-17'\n",
    "bucket_name = \"twitter-scrape-results\"\n",
    "key_name = \"test1.json\"\n",
    "\n",
    "# This represents the POST request data that gets sent to Lambda function\n",
    "data = {\n",
    "    \"query_string\": query_string,\n",
    "    \"bucket_name\": bucket_name,\n",
    "    \"key_name\": key_name\n",
    "}\n",
    "\n",
    "event = {\"body\": json.dumps(data)} # Only used for local testing\n",
    "resp = lambda_handler(event)\n",
    "len(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame(resp)\n",
    "df.to_excel(\"df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "def write_to_s3(bucket_name:str, key_name:str, upload_dict:dict) -> dict:\n",
    "    \"\"\"Upload Python dictionary to S3\n",
    "\n",
    "    Returns:\n",
    "        str: Amazon ETag that can be used to download the file.\n",
    "    \"\"\"\n",
    "    twitter_bytes = json.dumps(upload_dict).encode('utf-8')\n",
    "    s3 = boto3.resource('s3')\n",
    "    object = s3.Object(bucket_name, key_name)\n",
    "    return object.put(Body=twitter_bytes)\n",
    "\n",
    "\n",
    "def read_from_s3(bucket_name:str, key_name:str) -> dict:\n",
    "    s3 = boto3.resource('s3')\n",
    "    object = s3.Object(bucket_name, key_name)\n",
    "    response = object.get()\n",
    "    response_str = response[\"Body\"].read().decode(\"utf-8\")\n",
    "    dict_obj = json.loads(response_str)\n",
    "    return dict_obj\n",
    "\n",
    "\n",
    "bucket_name = \"twitter-scrape-results\"\n",
    "key_name = \"upload_test2.json\"\n",
    "\n",
    "upload_dict = twitter_dicts\n",
    "response = write_to_s3(bucket_name, key_name, upload_dict)\n",
    "\n",
    "\n",
    "download_dict1 = read_from_s3(bucket_name, key_name)\n",
    "download_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "def write_to_s3(bucket_name:str, key_name:str, upload_dict:dict) -> dict:\n",
    "    \"\"\"Upload Python dictionary to S3\n",
    "\n",
    "    Returns:\n",
    "        str: Amazon ETag that can be used to download the file.\n",
    "    \"\"\"\n",
    "    twitter_bytes = json.dumps(upload_dict).encode('utf-8')\n",
    "    s3 = boto3.resource('s3')\n",
    "    object = s3.Object(bucket_name, key_name)\n",
    "    return object.put(Body=twitter_bytes)\n",
    "\n",
    "bucket_name = \"twitter-scrape-results\"\n",
    "key_name = \"test_json_file.json\"\n",
    "\n",
    "response = write_to_s3(bucket_name, key_name, upload_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env39': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7b9a9ff273ba8d2808034d15347d54f87860ece42ba61f3e82cfd7b992977d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
